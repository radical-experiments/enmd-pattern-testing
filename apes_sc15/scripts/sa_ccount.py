#!/usr/bin/env python

"""
This example shows how to use the Ensemble MD Toolkit ``SimulationAnalysis``
pattern to execute 32 iterations of a simulation analysis loop. In the
``pre_loop`` step, a reference random ASCII file is uploaded. Each
``simulation_step`` generates 16 new random ASCII files.  Each ``analysis_step``
checks calculates the Levenshtein distance between  the newly generated files
and the reference file.

.. figure:: ../images/simulation_analysis_pattern.*
   :width: 300pt
   :align: center
   :alt: Simulation-Analysis Pattern

   Fig.: `The Simulation-Analysis Pattern.`

Run Locally
^^^^^^^^^^^

.. warning:: In order to run this example, you need access to a MongoDB server and
             set the ``RADICAL_PILOT_DBURL`` in your environment accordingly.
             The format is ``mongodb://hostname:port``. Read more about it
             MongoDB in chapter :ref:`envpreparation`.

**Step 1:** View and download the example sources :ref:`below <example_source_simulation_analysis_loop>`.

**Step 2:** Run this example with ``RADICAL_ENMD_VERBOSE`` set to ``info`` if you want to
see log messages about simulation progress::

    RADICAL_ENMD_VERBOSE=info python simulation_analysis_loop.py

Once the script has finished running, you should see the SHA1 checksums
generated by the individual ensembles  (``checksumXX.sha1``) in the in the same
directory you launched the script in.

Run Remotely
^^^^^^^^^^^^

By default, simulation and analysis steps run on one core your local machine::

    SingleClusterEnvironment(
        resource="localhost",
        cores=1,
        walltime=30,
        username=None,
        project=None
    )

You can change the script to use a remote HPC cluster and increase the number
of cores to see how this affects the runtime of the script as the individual
pipeline instances can run in parallel::

    SingleClusterEnvironment(
        resource="stampede.tacc.utexas.edu",
        cores=16,
        walltime=30,
        username=None,  # add your username here
        project=None # add your allocation or project id here if required
    )

.. _example_source_simulation_analysis_loop:

Example Source
^^^^^^^^^^^^^^
"""

__author__       = "Ole Weider <ole.weidner@rutgers.edu>"
__copyright__    = "Copyright 2014, http://radical.rutgers.edu"
__license__      = "MIT"
__example_name__ = "Simulation-Analysis Example (generic)"

import math
import pandas
import pickle
import pprint
import datetime

from radical.ensemblemd import Kernel
from radical.ensemblemd import SimulationAnalysisLoop
from radical.ensemblemd import EnsemblemdError
from radical.ensemblemd import SimulationAnalysisLoop
from radical.ensemblemd import SingleClusterEnvironment


# ------------------------------------------------------------------------------
#
class MSSA(SimulationAnalysisLoop):
    """RandomSA implements the simulation-analysis loop described above. It
       inherits from radical.ensemblemd.SimulationAnalysisLoop, the abstract
       base class for all Simulation-Analysis applications.
    """
    def __init__(self, maxiterations, simulation_instances=1, analysis_instances=1):
        SimulationAnalysisLoop.__init__(self, maxiterations, simulation_instances, analysis_instances)

    def pre_loop(self):
        """pre_loop is executed before the main simulation-analysis loop is
           started. In this example we create an initial 1 kB random ASCII file
           that we use as the reference for all analysis steps.
        """
        pass
    def simulation_step(self, iteration, instance):
        """The simulation step generates a 1 kB file containing random ASCII
           characters that is compared against the 'reference' file in the
           subsequent analysis step.
        """
        k = Kernel(name="misc.mkfile")
        k.arguments = ["--size=10000000", "--filename=asciifile.dat"]
        return k

    def analysis_step(self, iteration, instance):

        link_input_data = []
        for i in range(1,self.simlation_instances+1):
            link_input_data.append("$PREV_SIMULATION_INSTANCE_{instance}/asciifile.dat > asciifile-{instance}.dat".format(instance=i))

        k = Kernel(name="misc.ccount")
        k.arguments = ["--inputfile=asciifile.dat", "--outputfile=cfreqs.dat"]
        k.link_input_data = link_input_data
        k.download_output_data = "cfreqs.dat"
        k.cores = 1
        return k
	
    def post_loop(self):
        # post_loop is executed after the main simulation-analysis loop has
        # finished. In this example we don't do anything here.
        pass


# ------------------------------------------------------------------------------
#
if __name__ == "__main__":

    try:
        # Create a new static execution context with one resource and a fixed
        # number of cores and runtime.

	scale = [1,16,32,64,128]
        #scale = [16]

	for core_count in scale:
            #core_count = 1
            instance_count = core_count
            cluster = SingleClusterEnvironment(
                #resource="localhost",
                resource="xsede.stampede",
                #resource="futuregrid.india",
                cores=core_count,
                walltime=30,
                username="tg826231",
                #username="nrs76",
                #password="L1i2h5k1i5n*",
                project="TG-MCB090174",
                database_url='mongodb://ec2-54-221-194-147.compute-1.amazonaws.com:24242',
                database_name = 'myexps',
                queue = "development"
            )

            # Allocate the resources.
            allocate_start = datetime.datetime.now()
            cluster.allocate()
            allocate_end = datetime.datetime.now()

            # We set both the the simulation and the analysis step 'instances' to 16.
            # This means that 16 instances of the simulation step and 16 instances of
            # the analysis step are executed every iteration.
            pattern_create_start = datetime.datetime.now()
            randomsa = MSSA(maxiterations=1, simulation_instances=instance_count, analysis_instances=instance_count)
            pattern_create_end = datetime.datetime.now()

            run_start = datetime.datetime.now()
            cluster.run(randomsa)
            run_end = datetime.datetime.now()

            deallocate_start = datetime.datetime.now()
            cluster.deallocate()
            deallocate_end = datetime.datetime.now()

            client_enmd_overhead = (pattern_create_end - pattern_create_start).total_seconds() + \
                                    (run_end - run_start).total_seconds()
                                    

            randomsa.execution_profile_dict[2]['total_enmd_overhead'] += client_enmd_overhead



            df_string = "sa_results_cores_{0}_instances_{1}.pkl".format(core_count,instance_count)

            df = randomsa.execution_profile_dataframe
            pp = pprint.PrettyPrinter()
            pp.pprint(randomsa.execution_profile_dict)
            
            dict_string = "sa_results_cores_{0}_instances_{1}_dict.pkl".format(core_count,instance_count)
            pickle.dump(randomsa.execution_profile_dict,open(dict_string,"w"))
            df.to_pickle(df_string)


    except EnsemblemdError, er:

        print "Ensemble MD Toolkit Error: {0}".format(str(er))
        raise # Just raise the execption again to get the backtrace
